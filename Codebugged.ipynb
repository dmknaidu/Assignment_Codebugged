{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bac890f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8714d23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e858732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6fd711",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f23aa5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175d0f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Training Loss: 0.08367542177438736, Validation Loss: 0.08112664520740509, Validation RMSLE: 3.3421108722686768\n",
      "Epoch 2/50, Training Loss: 0.08129365742206573, Validation Loss: 0.07882606983184814, Validation RMSLE: 3.2943825721740723\n",
      "Epoch 3/50, Training Loss: 0.07899253815412521, Validation Loss: 0.07660184800624847, Validation RMSLE: 3.2475714683532715\n",
      "Epoch 4/50, Training Loss: 0.07676780223846436, Validation Loss: 0.07444833964109421, Validation RMSLE: 3.201596260070801\n",
      "Epoch 5/50, Training Loss: 0.07461380958557129, Validation Loss: 0.07236013561487198, Validation RMSLE: 3.156376361846924\n",
      "Epoch 6/50, Training Loss: 0.07252515107393265, Validation Loss: 0.07033269107341766, Validation RMSLE: 3.1118433475494385\n",
      "Epoch 7/50, Training Loss: 0.07049726694822311, Validation Loss: 0.06836258620023727, Validation RMSLE: 3.0679502487182617\n",
      "Epoch 8/50, Training Loss: 0.0685267224907875, Validation Loss: 0.06644774228334427, Validation RMSLE: 3.0246782302856445\n",
      "Epoch 9/50, Training Loss: 0.06661143898963928, Validation Loss: 0.06458744406700134, Validation RMSLE: 2.982037305831909\n",
      "Epoch 10/50, Training Loss: 0.06475067883729935, Validation Loss: 0.06278220564126968, Validation RMSLE: 2.940067768096924\n",
      "Epoch 11/50, Training Loss: 0.06294496357440948, Validation Loss: 0.06103368103504181, Validation RMSLE: 2.8988373279571533\n",
      "Epoch 12/50, Training Loss: 0.061195943504571915, Validation Loss: 0.059344515204429626, Validation RMSLE: 2.8584415912628174\n",
      "Epoch 13/50, Training Loss: 0.05950623378157616, Validation Loss: 0.05771823599934578, Validation RMSLE: 2.8190033435821533\n",
      "Epoch 14/50, Training Loss: 0.05787936970591545, Validation Loss: 0.056159209460020065, Validation RMSLE: 2.780670642852783\n",
      "Epoch 15/50, Training Loss: 0.056319717317819595, Validation Loss: 0.05467258766293526, Validation RMSLE: 2.743619441986084\n",
      "Epoch 16/50, Training Loss: 0.054832398891448975, Validation Loss: 0.053264256566762924, Validation RMSLE: 2.708051919937134\n",
      "Epoch 17/50, Training Loss: 0.053423307836055756, Validation Loss: 0.05194077268242836, Validation RMSLE: 2.674196243286133\n",
      "Epoch 18/50, Training Loss: 0.05209898576140404, Validation Loss: 0.050709228962659836, Validation RMSLE: 2.6423027515411377\n",
      "Epoch 19/50, Training Loss: 0.050866518169641495, Validation Loss: 0.04957704246044159, Validation RMSLE: 2.6126389503479004\n",
      "Epoch 20/50, Training Loss: 0.04973331093788147, Validation Loss: 0.048551641404628754, Validation RMSLE: 2.585479259490967\n",
      "Epoch 21/50, Training Loss: 0.0487067773938179, Validation Loss: 0.04763995110988617, Validation RMSLE: 2.561089277267456\n",
      "Epoch 22/50, Training Loss: 0.04779384657740593, Validation Loss: 0.04684777930378914, Validation RMSLE: 2.5397067070007324\n",
      "Epoch 23/50, Training Loss: 0.04700032249093056, Validation Loss: 0.0461789108812809, Validation RMSLE: 2.5215113162994385\n",
      "Epoch 24/50, Training Loss: 0.04632999747991562, Validation Loss: 0.04563409090042114, Validation RMSLE: 2.5065927505493164\n",
      "Epoch 25/50, Training Loss: 0.04578361287713051, Validation Loss: 0.04520981013774872, Validation RMSLE: 2.494913101196289\n",
      "Epoch 26/50, Training Loss: 0.045357681810855865, Validation Loss: 0.04489728435873985, Validation RMSLE: 2.4862747192382812\n",
      "Epoch 27/50, Training Loss: 0.04504343867301941, Validation Loss: 0.04468162730336189, Validation RMSLE: 2.4802963733673096\n",
      "Epoch 28/50, Training Loss: 0.04482605308294296, Validation Loss: 0.04454178363084793, Validation RMSLE: 2.4764115810394287\n",
      "Epoch 29/50, Training Loss: 0.044684477150440216, Validation Loss: 0.04445137083530426, Validation RMSLE: 2.4738969802856445\n",
      "Epoch 30/50, Training Loss: 0.04459242522716522, Validation Loss: 0.0443810299038887, Validation RMSLE: 2.4719390869140625\n",
      "Epoch 31/50, Training Loss: 0.04452057182788849, Validation Loss: 0.04430180415511131, Validation RMSLE: 2.4697318077087402\n",
      "Epoch 32/50, Training Loss: 0.04444000869989395, Validation Loss: 0.04418892413377762, Validation RMSLE: 2.466583251953125\n",
      "Epoch 33/50, Training Loss: 0.04432599991559982, Validation Loss: 0.04402489587664604, Validation RMSLE: 2.462000846862793\n",
      "Epoch 34/50, Training Loss: 0.04416109621524811, Validation Loss: 0.04380100592970848, Validation RMSLE: 2.455732822418213\n",
      "Epoch 35/50, Training Loss: 0.04393656551837921, Validation Loss: 0.04351711645722389, Validation RMSLE: 2.4477617740631104\n",
      "Epoch 36/50, Training Loss: 0.043652281165122986, Validation Loss: 0.04318005591630936, Validation RMSLE: 2.4382636547088623\n",
      "Epoch 37/50, Training Loss: 0.04331502690911293, Validation Loss: 0.04280119389295578, Validation RMSLE: 2.4275434017181396\n",
      "Epoch 38/50, Training Loss: 0.042936161160469055, Validation Loss: 0.042393967509269714, Validation RMSLE: 2.4159674644470215\n",
      "Epoch 39/50, Training Loss: 0.042529065161943436, Validation Loss: 0.041971608996391296, Validation RMSLE: 2.403902769088745\n",
      "Epoch 40/50, Training Loss: 0.04210694134235382, Validation Loss: 0.04154551029205322, Validation RMSLE: 2.3916690349578857\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from catboost import CatBoostRegressor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# RMSLE function\n",
    "def calculate_rmsle(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square(np.log1p(y_pred) - np.log1p(y_true))))\n",
    "\n",
    "# Load the data\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "stores = pd.read_csv('stores.csv')\n",
    "oil = pd.read_csv('oil.csv')\n",
    "holidays = pd.read_csv('holidays_events.csv')\n",
    "transactions = pd.read_csv('transactions.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# Preprocessing - Keep only data from 2016 onwards\n",
    "train['date'] = pd.to_datetime(train['date'])\n",
    "test['date'] = pd.to_datetime(test['date'])\n",
    "train = train[train['date'] >= '2016-01-01']\n",
    "\n",
    "# Adding features for year, month, day, and holidays\n",
    "train['holiday'] = train['date'].isin(holidays['date'])\n",
    "test['holiday'] = test['date'] == pd.to_datetime('2017-08-24')\n",
    "train['year'] = train['date'].dt.year\n",
    "train['month'] = train['date'].dt.month\n",
    "train['day'] = train['date'].dt.day\n",
    "train['weekday'] = train['date'].dt.weekday\n",
    "test['year'] = test['date'].dt.year\n",
    "test['month'] = test['date'].dt.month\n",
    "test['day'] = test['date'].dt.day\n",
    "test['weekday'] = test['date'].dt.weekday\n",
    "\n",
    "# Drop date for training and test sets\n",
    "train = train.drop(columns=['date'])\n",
    "test = test.drop(columns=['date'])\n",
    "\n",
    "# One-hot encoding for categorical columns\n",
    "object_cols = train.select_dtypes(include=['object']).columns\n",
    "train = pd.get_dummies(train, columns=object_cols, drop_first=True)\n",
    "test = pd.get_dummies(test, columns=object_cols, drop_first=True)\n",
    "\n",
    "# Align train and test sets\n",
    "train, test = train.align(test, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Features and target setup\n",
    "X = train.drop(columns=['sales'])\n",
    "y = train['sales']\n",
    "\n",
    "# Log transformation of the target to reduce the effect of outliers\n",
    "y_log = np.log1p(y)\n",
    "\n",
    "# Train-validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_log, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_val_scaled = scaler_X.transform(X_val)\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1))\n",
    "\n",
    "# Reshape data for LSTM input\n",
    "X_train_scaled = X_train_scaled.reshape(X_train_scaled.shape[0], 1, X_train_scaled.shape[1])\n",
    "X_val_scaled = X_val_scaled.reshape(X_val_scaled.shape[0], 1, X_val_scaled.shape[1])\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val_scaled, dtype=torch.float32)\n",
    "\n",
    "# Define the LSTM model in PyTorch\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)  \n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)  \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# LSTM Model parameters\n",
    "input_size = X_train_tensor.shape[2]\n",
    "hidden_size = 50\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "\n",
    "# Instantiate the LSTM model, define loss and optimizer\n",
    "lstm_model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the LSTM model\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    lstm_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_train_pred = lstm_model(X_train_tensor)\n",
    "    loss = loss_function(y_train_pred, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation step\n",
    "    lstm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_val_pred = lstm_model(X_val_tensor)\n",
    "        val_loss = loss_function(y_val_pred, y_val_tensor)\n",
    "        y_val_pred_inverse = scaler_y.inverse_transform(y_val_pred.numpy())\n",
    "        y_val_inverse = scaler_y.inverse_transform(y_val_tensor.numpy())\n",
    "        val_rmsle = calculate_rmsle(np.expm1(y_val_inverse), np.expm1(y_val_pred_inverse))\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation RMSLE: {val_rmsle}')\n",
    "\n",
    "# Extract features from LSTM\n",
    "with torch.no_grad():\n",
    "    lstm_train_features = lstm_model(X_train_tensor).numpy()\n",
    "    lstm_val_features = lstm_model(X_val_tensor).numpy()\n",
    "\n",
    "# Combine LSTM features with original dataset\n",
    "train_lstm_df = pd.DataFrame(lstm_train_features, columns=[f'lstm_feat_{i}' for i in range(lstm_train_features.shape[1])])\n",
    "val_lstm_df = pd.DataFrame(lstm_val_features, columns=[f'lstm_feat_{i}' for i in range(lstm_val_features.shape[1])])\n",
    "\n",
    "X_train_combined = pd.concat([X_train.reset_index(drop=True), train_lstm_df], axis=1)\n",
    "X_val_combined = pd.concat([X_val.reset_index(drop=True), val_lstm_df], axis=1)\n",
    "\n",
    "# Set up CatBoost model with fixed parameters (iterations=1000, depth=8, learning_rate=0.1)\n",
    "catboost_model = CatBoostRegressor(\n",
    "    iterations=1000,\n",
    "    depth=8,\n",
    "    learning_rate=0.1,\n",
    "    loss_function='RMSE',\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "# Train CatBoost model\n",
    "catboost_model.fit(X_train_combined, y_train, eval_set=(X_val_combined, y_val), early_stopping_rounds=50)\n",
    "\n",
    "# Assuming the previous preprocessing steps have been done\n",
    "\n",
    "# Prepare test dataset before scaling\n",
    "# Drop any columns that should not be in the test set\n",
    "test = test.drop(columns=['sales'], errors='ignore')  # Use errors='ignore' in case 'sales' is not present\n",
    "\n",
    "# Ensure alignment of test data features with training data\n",
    "test, _ = test.align(X_train, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Transform test data using the previously fitted scaler\n",
    "X_test_scaled = scaler_X.transform(test)  # Transform test data\n",
    "X_test_scaled = X_test_scaled.reshape(X_test_scaled.shape[0], 1, X_test_scaled.shape[1])\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "\n",
    "# Extract LSTM features for test data\n",
    "with torch.no_grad():\n",
    "    lstm_test_features = lstm_model(X_test_tensor).numpy()\n",
    "\n",
    "# Combine LSTM features with the original test features\n",
    "test_lstm_df = pd.DataFrame(lstm_test_features, columns=[f'lstm_feat_{i}' for i in range(lstm_test_features.shape[1])])\n",
    "X_test_combined = pd.concat([test.reset_index(drop=True), test_lstm_df], axis=1)\n",
    "\n",
    "# Make predictions on the test set with the CatBoost model\n",
    "test_predictions_catboost = catboost_model.predict(X_test_combined)\n",
    "\n",
    "# Convert predictions back from log1p scale\n",
    "test['sales'] = np.expm1(test_predictions_catboost)\n",
    "\n",
    "# Handle negative values in predictions (if any)\n",
    "test['sales'] = np.where(test['sales'] < 0, 0, test['sales'])\n",
    "\n",
    "# Prepare the submission file\n",
    "submission = test[['id', 'sales']]\n",
    "submission.to_csv('submission_new.csv', index=False)\n",
    "print(\"Submission file generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c65c649e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file generated.\n"
     ]
    }
   ],
   "source": [
    "# Assuming the previous preprocessing steps have been done\n",
    "\n",
    "# Prepare test dataset before scaling\n",
    "# Drop any columns that should not be in the test set\n",
    "test = test.drop(columns=['sales'], errors='ignore')  # Use errors='ignore' in case 'sales' is not present\n",
    "\n",
    "# Ensure alignment of test data features with training data\n",
    "test, _ = test.align(X_train, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Transform test data using the previously fitted scaler\n",
    "X_test_scaled = scaler_X.transform(test)  # Transform test data\n",
    "X_test_scaled = X_test_scaled.reshape(X_test_scaled.shape[0], 1, X_test_scaled.shape[1])\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "\n",
    "# Extract LSTM features for test data\n",
    "with torch.no_grad():\n",
    "    lstm_test_features = lstm_model(X_test_tensor).numpy()\n",
    "\n",
    "# Combine LSTM features with the original test features\n",
    "test_lstm_df = pd.DataFrame(lstm_test_features, columns=[f'lstm_feat_{i}' for i in range(lstm_test_features.shape[1])])\n",
    "X_test_combined = pd.concat([test.reset_index(drop=True), test_lstm_df], axis=1)\n",
    "\n",
    "# Make predictions on the test set with the CatBoost model\n",
    "test_predictions_catboost = catboost_model.predict(X_test_combined)\n",
    "\n",
    "# Convert predictions back from log1p scale\n",
    "test['sales'] = np.expm1(test_predictions_catboost)\n",
    "\n",
    "# Handle negative values in predictions (if any)\n",
    "test['sales'] = np.where(test['sales'] < 0, 0, test['sales'])\n",
    "\n",
    "# Prepare the submission file\n",
    "submission = test[['id', 'sales']]\n",
    "submission.to_csv('submission_new.csv', index=False)\n",
    "print(\"Submission file generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4867cdc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
